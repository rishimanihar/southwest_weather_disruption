# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dhVjwd7VaZ4kkhjjOSbcgpqXH5lZXO4A
"""

# -*- coding: utf-8 -*-
"""
Airline Disruption Prediction Pipeline
---------------------------------------
This script performs the entire data science pipeline:
1. Scrapes and cleans weather data (NOAA).
2. Filters and aggregates flight data (BTS).
3. Creates a Disruption Index using PCA.
4. Trains an XGBoost model to predict disruption.
"""

import pandas as pd
import numpy as np
import requests
import os
import glob
import sys
import csv
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

# ==========================================
# CONFIGURATION
# ==========================================
# NOTE: We use relative paths for GitHub.
# Ensure you have a folder named 'data' in the same directory.
DATA_DIR = 'data'
WEATHER_RAW_DIR = os.path.join(DATA_DIR, 'hourly_weather_raw')
BTS_RAW_DIR = os.path.join(DATA_DIR, 'bts_delay_data')

# Create directories if they don't exist
os.makedirs(WEATHER_RAW_DIR, exist_ok=True)
os.makedirs(BTS_RAW_DIR, exist_ok=True)

# Files
STATIONS_FILE = os.path.join(DATA_DIR, 'isd-history.csv')
FILTERED_STATIONS_FILE = os.path.join(DATA_DIR, 'sw_stations.csv')
WEATHER_MASTER_FILE = os.path.join(DATA_DIR, 'weather_master_cleaned.csv')
BTS_MASTER_FILE = os.path.join(DATA_DIR, 'bts_master_filtered.csv')
DISRUPTION_HOURLY_FILE = os.path.join(DATA_DIR, 'disruption_index_hourly.csv')
DISRUPTION_PCA_FILE = os.path.join(DATA_DIR, 'disruption_index_PCA.csv')
FINAL_PREDICTIONS_FILE = os.path.join(DATA_DIR, 'final_model_predictions.csv')

# Target Airports
TARGET_AIRPORTS = [
    "DEN", "LAS", "MDW", "BWI", "DAL", "ATL", "PHX", "HOU", "MCO",
    "LAX", "BNA", "OAK", "STL", "SAN", "TPA", "AUS", "FLL", "MSY",
    "SJC", "SMF", "BOS", "MCI", "SAT", "SNA", "RDU"
]

# Increase CSV limit for large BTS files
try:
    csv.field_size_limit(sys.maxsize)
except OverflowError:
    csv.field_size_limit(2147483647)


# ==========================================
# PHASE 1: WEATHER DATA ACQUISITION
# ==========================================
def fetch_weather_data():
    print("\n--- PHASE 1: Fetching Weather Data ---")

    # 1. Filter Stations
    if not os.path.exists(STATIONS_FILE):
        print(f"ERROR: {STATIONS_FILE} not found. Please download isd-history.csv to the data folder.")
        return

    sw_stations_df = pd.read_csv(STATIONS_FILE)
    sw_stations_df = sw_stations_df[sw_stations_df['ICAO'].isin([f"K{code}" for code in TARGET_AIRPORTS])]

    # Save filtered list
    sw_stations_df[['USAF', 'WBAN', 'ICAO', 'STATION NAME']].to_csv(FILTERED_STATIONS_FILE, index=False)

    # 2. Download Hourly Data
    sw_stations_df['WBAN'] = sw_stations_df['WBAN'].fillna(99999)
    sw_stations_df['USAF'] = sw_stations_df['USAF'].astype(str).str.split('.').str[0]
    sw_stations_df['WBAN'] = sw_stations_df['WBAN'].astype(str).str.split('.').str[0].str.zfill(5)

    base_url = 'https://www.ncei.noaa.gov/data/global-hourly/access/'
    years = [2022, 2023, 2024]

    print("Starting download loop...")
    for index, row in sw_stations_df.iterrows():
        station_id = f"{row['USAF']}{row['WBAN']}"
        icao_code = row['ICAO']

        for year in years:
            file_url = f"{base_url}{year}/{station_id}.csv"
            output_path = os.path.join(WEATHER_RAW_DIR, f"{icao_code}_{year}.csv")

            if os.path.exists(output_path):
                continue

            try:
                print(f"Downloading {icao_code} ({year})...")
                response = requests.get(file_url)
                if response.status_code == 200:
                    with open(output_path, 'wb') as f:
                        f.write(response.content)
            except Exception as e:
                print(f"Error downloading {icao_code}: {e}")

def parse_noaa_data(df):
    # Helper to parse NOAA specific formats
    # 1. Temperature
    temp_str = df['TMP'].astype(str).str.split(',', expand=True)[0]
    df['temp_c'] = pd.to_numeric(temp_str, errors='coerce') / 10.0
    df.loc[df['temp_c'] > 100, 'temp_c'] = np.nan

    # 2. Dew Point
    if 'DEW' in df.columns:
        dew_str = df['DEW'].astype(str).str.split(',', expand=True)[0]
        df['dew_point_c'] = pd.to_numeric(dew_str, errors='coerce') / 10.0
        df.loc[df['dew_point_c'] > 100, 'dew_point_c'] = np.nan

    # 3. Wind
    wnd_str = df['WND'].astype(str).str.split(',', expand=True)[3]
    df['wind_speed_ms'] = pd.to_numeric(wnd_str, errors='coerce') / 10.0
    df.loc[df['wind_speed_ms'] > 100, 'wind_speed_ms'] = np.nan

    # 4. Visibility
    vis_str = df['VIS'].astype(str).str.split(',', expand=True)[0]
    df['visibility_m'] = pd.to_numeric(vis_str, errors='coerce')
    df.loc[df['visibility_m'] > 100000, 'visibility_m'] = np.nan

    # 5. Ceiling
    if 'CIG' in df.columns:
        cig_str = df['CIG'].astype(str).str.split(',', expand=True)[0]
        df['ceiling_m'] = pd.to_numeric(cig_str, errors='coerce')
        df.loc[df['ceiling_m'] == 99999, 'ceiling_m'] = 30000

    # 6. Precip
    if 'AA1' in df.columns:
        precip_str = df['AA1'].astype(str).str.split(',', expand=True)[1]
        df['precip_depth_mm'] = pd.to_numeric(precip_str, errors='coerce') / 10.0
        df['precip_depth_mm'] = df['precip_depth_mm'].fillna(0)
    else:
        df['precip_depth_mm'] = 0.0

    # 7. Present Weather (One-Hot)
    df['is_fog'] = 0
    df['is_rain'] = 0
    df['is_snow'] = 0
    df['is_thunder'] = 0

    if 'AW1' in df.columns:
        aw_code = df['AW1'].astype(str).str.split(',', expand=True)[0]
        aw_code = pd.to_numeric(aw_code, errors='coerce')

        df.loc[(aw_code >= 10) & (aw_code <= 49), 'is_fog'] = 1
        df.loc[(aw_code >= 50) & (aw_code <= 69), 'is_rain'] = 1
        df.loc[(aw_code >= 80) & (aw_code <= 84), 'is_rain'] = 1
        df.loc[(aw_code >= 70) & (aw_code <= 79), 'is_snow'] = 1
        df.loc[(aw_code >= 85) & (aw_code <= 89), 'is_snow'] = 1
        df.loc[aw_code >= 90, 'is_thunder'] = 1

    return df[['DATE', 'temp_c', 'dew_point_c', 'wind_speed_ms', 'visibility_m',
               'ceiling_m', 'precip_depth_mm', 'is_fog', 'is_rain', 'is_snow', 'is_thunder']]

def process_weather_data():
    print("\n--- Processing Weather Data ---")
    all_data = []
    files = [f for f in os.listdir(WEATHER_RAW_DIR) if f.endswith('.csv')]

    for filename in files:
        try:
            icao_code = filename.split('_')[0]
            file_path = os.path.join(WEATHER_RAW_DIR, filename)
            raw_df = pd.read_csv(file_path, low_memory=False)
            clean_df = parse_noaa_data(raw_df)
            clean_df["airport_code"] = icao_code
            all_data.append(clean_df)
        except Exception as e:
            print(f"Error processing {filename}: {e}")

    if all_data:
        master_weather = pd.concat(all_data, ignore_index=True)
        master_weather['timestamp'] = pd.to_datetime(master_weather['DATE'])
        master_weather['timestamp'] = master_weather['timestamp'].dt.floor('h')
        master_weather = master_weather.drop(columns=['DATE'])

        # Group by hour to remove duplicates
        final_df = master_weather.groupby(['airport_code', 'timestamp']).mean().reset_index()

        # Impute missing values
        final_df = final_df.sort_values(by=['airport_code', 'timestamp'])
        final_df = final_df.ffill().fillna(0)

        final_df.to_csv(WEATHER_MASTER_FILE, index=False)
        print(f"Saved cleaned weather master to {WEATHER_MASTER_FILE}")
    else:
        print("No weather data processed.")

# ==========================================
# PHASE 2: FLIGHT DATA PREP
# ==========================================
def process_bts_data():
    print("\n--- PHASE 2: Processing Flight Data ---")
    file_paths = glob.glob(os.path.join(BTS_RAW_DIR, '*.zip')) + glob.glob(os.path.join(BTS_RAW_DIR, '*.csv'))

    if not file_paths:
        print(f"No BTS files found in {BTS_RAW_DIR}.")
        return

    columns_to_keep = ['FL_DATE', 'OP_UNIQUE_CARRIER', 'ORIGIN', 'CRS_DEP_TIME', 'ARR_DELAY', 'CANCELLED', 'DIVERTED']
    all_data = []

    for file_path in file_paths:
        print(f"Processing {os.path.basename(file_path)}...")
        try:
            df = pd.read_csv(file_path, usecols=columns_to_keep, low_memory=False)
            df = df[df['OP_UNIQUE_CARRIER'] == 'WN'] # Southwest only
            df = df[df['ORIGIN'].isin(TARGET_AIRPORTS)]
            all_data.append(df)
        except Exception as e:
            print(f"Error reading {file_path}: {e}")

    if all_data:
        master_df = pd.concat(all_data, ignore_index=True)
        master_df.to_csv(BTS_MASTER_FILE, index=False)
        print(f"Saved filtered BTS master to {BTS_MASTER_FILE}")

        # Aggregate by Hour
        print("Aggregating by hour...")
        date_col = pd.to_datetime(master_df['FL_DATE'], errors='coerce')
        date_str = date_col.dt.strftime('%Y-%m-%d')
        time_str = master_df['CRS_DEP_TIME'].astype(str).str.split('.').str[0].str.zfill(4).replace('2400', '2359')

        master_df['timestamp'] = pd.to_datetime(date_str + ' ' + time_str, format='%Y-%m-%d %H%M', errors='coerce')
        master_df = master_df.dropna(subset=['timestamp'])
        master_df['timestamp_h'] = master_df['timestamp'].dt.floor('h')

        grouped = master_df.groupby(['ORIGIN', 'timestamp_h'])
        agg_df = grouped.agg(
            avg_arr_delay=('ARR_DELAY', 'mean'),
            percent_cancelled=('CANCELLED', 'mean'), # Fixed typo from your original file
            percent_diverted=('DIVERTED', 'mean')
        ).reset_index()

        agg_df.to_csv(DISRUPTION_HOURLY_FILE, index=False)
        print(f"Saved hourly disruption index to {DISRUPTION_HOURLY_FILE}")

        # Run PCA
        print("Running PCA...")
        agg_df = agg_df.dropna()
        metrics = agg_df[['avg_arr_delay', 'percent_cancelled', 'percent_diverted']]
        scaler = StandardScaler()
        metrics_scaled = scaler.fit_transform(metrics)

        pca = PCA(n_components=1)
        agg_df['Disruption_Index'] = pca.fit_transform(metrics_scaled)

        # Flip sign if correlated negatively with delay
        if pca.components_[0][0] < 0:
            agg_df['Disruption_Index'] = -agg_df['Disruption_Index']

        final_df = agg_df[['ORIGIN', 'timestamp_h', 'Disruption_Index']]
        final_df = final_df.rename(columns={'ORIGIN': 'airport_code', 'timestamp_h': 'timestamp'})
        final_df.to_csv(DISRUPTION_PCA_FILE, index=False)
        print(f"Saved final PCA target to {DISRUPTION_PCA_FILE}")

# ==========================================
# PHASE 3: MODELING
# ==========================================
def train_model():
    print("\n--- PHASE 3: Training Model ---")
    if not os.path.exists(WEATHER_MASTER_FILE) or not os.path.exists(DISRUPTION_PCA_FILE):
        print("Missing master files. Run Phase 1 and 2 first.")
        return

    df_weather = pd.read_csv(WEATHER_MASTER_FILE)
    df_disruption = pd.read_csv(DISRUPTION_PCA_FILE)

    # Standardize airport codes
    df_weather['airport_code'] = df_weather['airport_code'].str.lstrip('K')

    # Parse timestamps
    df_weather['timestamp'] = pd.to_datetime(df_weather['timestamp'])
    df_disruption['timestamp'] = pd.to_datetime(df_disruption['timestamp'])

    # Feature Engineering
    print("Engineering Features...")
    df_weather['hour'] = df_weather['timestamp'].dt.hour
    df_weather['month'] = df_weather['timestamp'].dt.month
    df_weather['day_of_week'] = df_weather['timestamp'].dt.dayofweek
    df_weather = df_weather.sort_values(['airport_code', 'timestamp'])

    # Lags
    lags = ['wind_speed_ms', 'visibility_m', 'precip_depth_mm', 'is_thunder', 'is_snow']
    for col in lags:
        df_weather[f'{col}_lag1'] = df_weather.groupby('airport_code')[col].shift(1)
        df_weather[f'{col}_lag3'] = df_weather.groupby('airport_code')[col].shift(3)

    # Rolling Windows
    rolling = ['wind_speed_ms', 'precip_depth_mm', 'is_snow', 'is_thunder']
    for col in rolling:
        df_weather[f'{col}_roll6'] = df_weather.groupby('airport_code')[col].transform(lambda x: x.rolling(6, min_periods=1).mean())
        df_weather[f'{col}_roll12'] = df_weather.groupby('airport_code')[col].transform(lambda x: x.rolling(12, min_periods=1).mean())

    # Merge
    print("Merging Data...")
    df_master = pd.merge(df_weather, df_disruption, on=['airport_code', 'timestamp'], how='inner')

    # One-Hot Encoding
    df_master = pd.get_dummies(df_master, columns=['airport_code'], prefix='airport')
    df_master = df_master.dropna()

    # Define Features
    exclude = ['timestamp', 'Disruption_Index']
    features = [c for c in df_master.columns if c not in exclude]
    target = 'Disruption_Index'

    X = df_master[features]
    y = df_master[target]

    # Split
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

    # Train
    print(f"Training XGBoost on {len(X_train)} records...")
    model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, n_jobs=-1, random_state=42)
    model.fit(X_train, y_train)

    # Evaluate
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f"Results -> R2: {r2:.3f}, RMSE: {rmse:.3f}")

    # Save 0-100 Score
    scaler = MinMaxScaler(feature_range=(0, 100))
    scaler.fit(y_train.values.reshape(-1, 1))
    y_pred_scaled = scaler.transform(y_pred.reshape(-1, 1))

    results = pd.DataFrame({
        'timestamp': df_master.iloc[split_idx:]['timestamp'],
        'Raw_Prediction': y_pred,
        'Final_Score_0_100': y_pred_scaled.flatten()
    })
    results.to_csv(FINAL_PREDICTIONS_FILE, index=False)
    print(f"Final predictions saved to {FINAL_PREDICTIONS_FILE}")

# ==========================================
# MAIN EXECUTION
# ==========================================
if __name__ == "__main__":
    # Uncomment the steps you want to run:
    fetch_weather_data()   # Step 1
    process_weather_data() # Step 2
    process_bts_data()     # Step 3
    train_model()          # Step 4
    print("\nPipeline Complete.")